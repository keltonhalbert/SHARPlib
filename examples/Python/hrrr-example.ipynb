{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f886fb3-a3c1-4762-8414-6a73de4553c7",
   "metadata": {},
   "source": [
    "# **SHARPlib -- Working with Gridded Data (HRRR Example)**\n",
    "\n",
    "This tutorial notebook provides an example of how to use SHARPlib with 3D gridded data sources, such as the **High Resolution Rapid Refresh (HRRR)** model. This notebook leverages some advanced tools and topics, such as [reading remote GRIB2 data using `kerchunk`](https://nbviewer.org/gist/peterm790/92eb1df3d58ba41d3411f8a840be2452), and [parallelizing computations on chunked arrays using Dask](https://tutorial.dask.org/02_array.html). For the sake of brevity, this notebook will not go over these in any detail, but rather, show how they can be used with SHARPlib. \n",
    "\n",
    "## HRRR Data\n",
    "JSON reference files that map to a single GRIB2 file on Google Cloud are provided in this repository to provide access as a virtual Zarr store. There are 3 separate files for 3 separate groups of variables on different coordinate systems: `hrrr-hybrid.json` for hybrid vertical level data, `hrrr-2m.json` for fields defined at 2-meters AGL, and `hrrr-surface.json` for variables defined on the model surface. The surface pressure is read from the surface group, 2-meter temperature and specific humidity are read from the 2-meter group, and the remaining 3D data `[pressure, geopotential height, temperature, specific humidity]` are read from the hybrid level group. \n",
    "\n",
    "## Dask Client\n",
    "A Dask client is used to set up a \"local cluster\" using your computer's CPUs. ***It is highly recommended that you rude the `n_workers` and `memory_limit` parameters to something appropriate for your system***. Setting `n_workers` or `memory_limit` to values that exceed what your system supports can cause significant slowdowns and even crashes.\n",
    "\n",
    "Once the client starts, you can click on the generated URL to see how your parallel tasks are being executed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7985aa-4d76-43d3-a001-674f0406b224",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dask throws some noise about \n",
    "## large task graphs. This is unavoidable\n",
    "## and clutters the output, so turn it off\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "import dask\n",
    "import logging\n",
    "import cmocean\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "from distributed import Client\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from cartopy import feature as cfeature\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "\n",
    "from nwsspc.sharp.calc import constants\n",
    "from nwsspc.sharp.calc import parcel\n",
    "from nwsspc.sharp.calc import params\n",
    "from nwsspc.sharp.calc import thermo\n",
    "from nwsspc.sharp.calc import winds\n",
    "from nwsspc.sharp.calc import layer\n",
    "from nwsspc.sharp.calc import interp\n",
    "\n",
    "client = Client(\n",
    "    n_workers=8, \n",
    "    memory_limit='8GB', \n",
    "    silence_logs=logging.ERROR\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fa3ddc-6d55-4800-ac94-894816352ee8",
   "metadata": {},
   "source": [
    "## Dataset Chunking\n",
    "One of the ways we can control performance and parallelism is by controling how the dataset is chunked. The chunk sizes control things like how much memory each task uses, how many tasks are generated, and task performance depending on how CPU cache-friendly the chunk strategy is.  \n",
    "\n",
    "First and foremost, the `hybrid` coordinate should be one, single chunk and not sub-divided. Our calculations with SHARPlib rely on contiguous, vertical profile arrays of data for computation, and splitting these into chunks in the vertical would break that requirement. This is effectively hard-coded in the call: `.chunk(dict(hybrid=-1, **chunks))`. \n",
    "\n",
    "Second, CPUs work most efficiently with linear, cache friendly data. Arrays are typically stored in memory using the C memory layout, meaning that an array shape of `(NZ, NY, NX)` is ordered linearly by `X` values. This means that, generally speaking, we can be the most efficient with our CPU cache by passing entire rows of data for computation, and chunking by our column coordinate `y`. The `-1` is shorthand for \"use the entire dimension\". \n",
    "\n",
    "Depending on your system, number of cores, and amount of memory, you may want to adjust these values. I recommend adjusting the y-chunk first before x, but if you do adjust x, make sure its sufficiently large for the CPU to leverage linearity! If you don't quite believe it, experiment with what happens when you use a small x-chunk and a large y-chunk, and compare the timing...\n",
    "\n",
    "## Dataset Precision and Downloading\n",
    "SHARPlib works using 32-bit floating point precision, while most of Python operates at 64-bit by default. A call to `.astype(\"float32\")` is made in order to convert all of the arrays to 32-bit precision up front. Additionally, a call to `.compute()` is included in order to download the dataset into memory. While Dask would and could handle downloading the data during later computation, sometimes it is simpler to just retain the whole dataset in memory -- particularly if additional computation is desired. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1d541a-e359-4c64-9ac3-f73bdc25c5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = {\"y\": 64, \"x\": 256}\n",
    "\n",
    "ds_hybrid = xr.open_dataset(\n",
    "    \"hrrr-hybrid.json\", \n",
    "    engine=\"kerchunk\", \n",
    "    decode_timedelta=True\n",
    ")[[\"pres\", \"gh\", \"t\", \"q\", \"u\", \"v\"]] \\\n",
    "    .astype(\"float32\") \\\n",
    "    .compute() \\\n",
    "    .chunk(dict(hybrid=-1, **chunks)) \n",
    "\n",
    "\n",
    "ds_2m = xr.open_dataset(\n",
    "    \"hrrr-2m.json\", \n",
    "    engine=\"kerchunk\", \n",
    "    decode_timedelta=True\n",
    ")[[\"t2m\", \"sh2\"]] \\\n",
    "    .astype(\"float32\") \\\n",
    "    .compute() \\\n",
    "    .chunk(chunks) \n",
    "\n",
    "\n",
    "ds_sfc = xr.open_dataset(\n",
    "    \"hrrr-surface.json\", \n",
    "    engine=\"kerchunk\", \n",
    "    decode_timedelta=True\n",
    ")[[\"sp\", \"orog\"]] \\\n",
    "    .astype(\"float32\") \\\n",
    "    .compute() \\\n",
    "    .chunk(chunks) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24e02fa-810d-4d59-8a5e-b716ff84affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_hybrid)\n",
    "print(\"==========\")\n",
    "print(ds_2m)\n",
    "print(\"==========\")\n",
    "print(ds_sfc)\n",
    "print(\"==========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb15d70-5148-4d87-9c3c-860a2fc4e678",
   "metadata": {},
   "source": [
    "## Calling SHARPlib on Gridded Data\n",
    "SHARPlib is designed to work on vertical arrays of profile data (as mentioned in the chunking section), but our model data is a combination of 2D and 3D fields. There is also a need for computing additional array data, such as `water vapor mixing ratio`, `virtual temperature` and `dewpoint temperature`. While the afformentioned variables could reasonably be pre-computed on the 3D grids at a relatively low computational cost, other things such as `CAPE`, `CINH`, and the `Effective Inflow Layer` are much more computationally expensive... and iterating over the HRRR grid multiple times to compute each variable is usually more expensive than just computing everything at once. Once a gridpoint's vertical profile is loaded into memory, why not go ahead and get everything you need out of it?\n",
    "<hr>\n",
    "\n",
    "### Function Arguments\n",
    "The function arguments are going to be structured like so...\n",
    "\n",
    "- 1D Vertical Arrays: `[pres, hght, tmpk, spfh]`<br>\n",
    "- Scalar values: `[sp, t2m, sh2]`<br>\n",
    "- Keyword arguments: `use_2m=True`<br>\n",
    "\n",
    "The magic of how we get 1D and scalar values out of 3D and 2D arrays is elaborated on further in the next cell. The primary goal is to show that computational logic should be locially grouped into a \"per-profile\" basis. \n",
    "\n",
    "### SHARPlib Computations\n",
    "Within this function, we compute and return the following:\n",
    "\n",
    "- **Computed**\n",
    "    - Water Vapor Mixing Ratio\n",
    "    - Virtual Temperature \n",
    "    - Dewpoint Temperature\n",
    "<br>\n",
    "\n",
    "- **Computed and Returned**\n",
    "    - Most Unstable Parcel CAPE\n",
    "    - Most Unstable Parcel CINH\n",
    "    - Effective Inflow Layer Bottom \n",
    "    - Effective Inflow Layer Top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae71bbe-90a4-40ac-bf7d-ad2a10dac365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_everything(\n",
    "    pres, hght, \n",
    "    tmpk, spfh, \n",
    "    uwin, vwin, \n",
    "    sp, orog, \n",
    "    t2m, sh2, \n",
    "):\n",
    "    def qc(val):\n",
    "        if (val == constants.MISSING): return np.nan\n",
    "        else: return val\n",
    "            \n",
    "    sp = sp.item()\n",
    "    orog = orog.item()\n",
    "    sh2 = sh2.item()\n",
    "    t2m = t2m.item()\n",
    "    \n",
    "    mixr = thermo.mixratio(spfh)\n",
    "    theta = thermo.theta(pres, tmpk)\n",
    "\n",
    "    mixr_2m = thermo.mixratio(sh2)\n",
    "    dwpk_2m = thermo.temperature_at_mixratio(mixr_2m, sp)\n",
    "\n",
    "    mixr[mixr < constants.TOL] = constants.TOL\n",
    "    if (mixr_2m < constants.TOL): mixr_2m = constants.TOL\n",
    "\n",
    "    vtmp = thermo.virtual_temperature(tmpk, mixr)\n",
    "    dwpk = thermo.temperature_at_mixratio(mixr, pres)\n",
    "\n",
    "    # get the mixed-layer parcel\n",
    "    mix_lyr = layer.PressureLayer(\n",
    "        pres[0], pres[0] - 10000.0)\n",
    "    mlpcl = parcel.Parcel.mixed_layer_parcel(\n",
    "        mix_lyr,\n",
    "        pres,\n",
    "        theta,\n",
    "        mixr\n",
    "    )\n",
    "\n",
    "    lifter = parcel.lifter_cm1()\n",
    "    lifter.ma_type = thermo.adiabat.pseudo_liq\n",
    "    lifter.converge = 0.15\n",
    "    \n",
    "    # lift the parcel and get CAPE\n",
    "    pcl_vtmp = mlpcl.lift_parcel(lifter, pres)\n",
    "    pcl_buoy = thermo.buoyancy(pcl_vtmp, vtmp)\n",
    "    cape, cinh = mlpcl.cape_cinh(pres, hght, pcl_buoy)\n",
    "\n",
    "        # Get the LCL height in meters AGL\n",
    "    mllcl_hght = interp.interp_pressure(\n",
    "        mlpcl.lcl_pressure,\n",
    "        pres,\n",
    "        hght\n",
    "    ) - orog\n",
    "\n",
    "    # Get the effective inflow layer for effective SRH\n",
    "    mupcl = parcel.Parcel()\n",
    "    eil = params.effective_inflow_layer(\n",
    "        lifter,\n",
    "        pres,\n",
    "        hght,\n",
    "        tmpk,\n",
    "        dwpk,\n",
    "        vtmp,\n",
    "        mupcl=mupcl\n",
    "    )\n",
    "    \n",
    "    eil_hght = layer.pressure_layer_to_height(\n",
    "        eil,\n",
    "        pres,\n",
    "        hght\n",
    "    )\n",
    "\n",
    "    # Get the storm relative helicity \n",
    "    # for the effective inflow layer\n",
    "    storm_mtn = params.storm_motion_bunkers(\n",
    "        pres,\n",
    "        hght,\n",
    "        uwin,\n",
    "        vwin,\n",
    "        eil, mupcl\n",
    "    )\n",
    "    esrh = winds.helicity(\n",
    "        eil,\n",
    "        storm_mtn,\n",
    "        pres,\n",
    "        uwin,\n",
    "        vwin\n",
    "    )\n",
    "\n",
    "    if mupcl.eql_pressure != constants.MISSING:\n",
    "        # Get the effective bulk wind difference\n",
    "        eql_hght = interp.interp_pressure(\n",
    "            mupcl.eql_pressure,\n",
    "            pres,\n",
    "            hght\n",
    "        )\n",
    "        depth = (eql_hght - eil_hght.bottom)*0.5\n",
    "        ebwd_lyr = layer.HeightLayer(\n",
    "            eil_hght.bottom, \n",
    "            float(eil_hght.bottom + depth)\n",
    "        )\n",
    "        ebwd_cmp = winds.wind_shear(\n",
    "            ebwd_lyr,\n",
    "            hght,\n",
    "            uwin,\n",
    "            vwin\n",
    "        )\n",
    "        ebwd = winds.vector_magnitude(ebwd_cmp.u, ebwd_cmp.v)\n",
    "    else:\n",
    "        ebwd_cmp = winds.WindComponents(constants.MISSING, constants.MISSING)\n",
    "        ebwd = 0\n",
    "\n",
    "\n",
    "    \n",
    "    stp = params.significant_tornado_parameter(\n",
    "        mlpcl,\n",
    "        mllcl_hght,\n",
    "        esrh,\n",
    "        ebwd\n",
    "    )\n",
    "\n",
    "    return mlpcl.cape, mlpcl.cinh, stp, qc(eil_hght.bottom), qc(eil_hght.top), qc(esrh), qc(ebwd_cmp.u), qc(ebwd_cmp.v)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5899bc6-beb3-42e3-ac62-da43fb10ac0a",
   "metadata": {},
   "source": [
    "## Parallelizing Computations with Xarray and Dask\n",
    "This is the magic of how to parallelize the profile-based computation across an entire gridded dataset. It relies on two key variables: `input_core_dims` and `output_core_dims` that tell Xarray about how to decompose the input and reconstruct the output.\n",
    "\n",
    "### Input Core Dims\n",
    "This is a list of coordinates for each variable, ordered by argument order. The first 4 arguments to `compute_everything` are the 1D arrays that we need to get from the 3D fields, and so the first four values to `input_core_dims` are `[\"hybrid\"]`. The `hybrid` dimension is our vertical dimension from the GRIB2 file, so, we are telling Xarray that this dimension is \"core\" to our computation, and should be present for these arguments/arrays in our function. \n",
    "\n",
    "### Output Core Dims\n",
    "Similarly to `input_core_dims`, `output_core_dims` tells Xarray how to reconstruct the returned values from our output. In the case of `compute_everything`, we are returning 6 scalar values that should be assembled as a 2D array. They have no \"core\" dimensions, since they are independent of any other neighbor or coordinate, so this is a list of empty lists. However, if we wanted to return a 3D array of parcel virtual temperature, we could add another output argument that would have an core dim of `[\"hybrid\"]`, much like the inputs. \n",
    "\n",
    "### Output DTypes\n",
    "Having the same length as the number of output variables, this tells Xarray the expected return type and precision. SHARPlib returns float32 types, so that's what we specify. \n",
    "\n",
    "### Other arguments\n",
    "The `vectorize=True` and `dask=\"parallelized` are required arguments in order to make this work. It coerces the function inputs to be numpy arrays (required to work with SHARPlib), and tells Dask that we want it to parallelize it in such a way that values such as `sp, t2m,` and `sh2` are scalars and not arrays. \n",
    "\n",
    "### Compute \n",
    "The return type from `apply_ufunc` is an object that tells Dask how to parallelize, but it does not compute a result intil `.compute()` is called. We do this for all of our variables at once so that we do not iterate over the grid multiple times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaef9d2-38f7-40a9-ac49-d70302ae7dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "input_core_dims = [\n",
    "    [\"hybrid\"], [\"hybrid\"],\n",
    "    [\"hybrid\"], [\"hybrid\"],\n",
    "    [\"hybrid\"], [\"hybrid\"],\n",
    "    [\"surface\"], [\"surface\"],\n",
    "    [\"heightAboveGround\"], \n",
    "    [\"heightAboveGround\"],\n",
    "]\n",
    "\n",
    "output_core_dims = [\n",
    "    [],[],\n",
    "    [],[],\n",
    "    [],[],\n",
    "    [],[],\n",
    "]\n",
    "\n",
    "output_dtypes = [\n",
    "    np.float32, np.float32, \n",
    "    np.float32, np.float32,\n",
    "    np.float32, np.float32,\n",
    "    np.float32, np.float32,\n",
    "]\n",
    "\n",
    "result_func = xr.apply_ufunc(\n",
    "    compute_everything, \n",
    "    ds_hybrid[\"pres\"], ds_hybrid[\"gh\"],\n",
    "    ds_hybrid[\"t\"], ds_hybrid[\"q\"],\n",
    "    ds_hybrid[\"u\"], ds_hybrid[\"v\"],\n",
    "    ds_sfc[\"sp\"], ds_sfc[\"orog\"],\n",
    "    ds_2m[\"t2m\"], ds_2m[\"sh2\"],\n",
    "    input_core_dims=input_core_dims,\n",
    "    output_core_dims=output_core_dims,\n",
    "    vectorize=True,\n",
    "    dask=\"parallelized\",\n",
    "    output_dtypes=output_dtypes,\n",
    ")\n",
    "\n",
    "output_names = [\n",
    "    \"mixed_layer_cape\", \"mixed_layer_cinh\", \n",
    "    \"effective_layer_stp\", \"effective_inflow_layer_height_bottom_agl\",\n",
    "    \"effective_inflow_layer_height_top_agl\", \"effective_srh\",\n",
    "    \"effective_bulk_wind_difference_u\", \"effective_bulk_wind_difference_v\",\n",
    "]\n",
    "\n",
    "result_dict = {name: data for name, data in zip(output_names, result_func)}\n",
    "result_ds = xr.Dataset(result_dict)\n",
    "result_ds = result_ds.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a693ed31-718c-4db0-be9f-53c60a7801de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_ds)\n",
    "print(result_ds[\"effective_layer_stp\"].max(), result_ds[\"effective_layer_stp\"].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab692de-ed92-4fff-afcc-c052a19b59d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lon = ds_2m.longitude.values\n",
    "lat = ds_2m.latitude.values\n",
    "\n",
    "stp = result_ds[\"effective_layer_stp\"]\n",
    "ml_cape = result_ds[\"mixed_layer_cape\"]\n",
    "ml_cinh = result_ds[\"mixed_layer_cinh\"]\n",
    "ebwd_u = result_ds[\"effective_bulk_wind_difference_u\"]*1.94384\n",
    "ebwd_v = result_ds[\"effective_bulk_wind_difference_v\"]*1.94384\n",
    "\n",
    "stp = gaussian_filter(stp, sigma=2.5)\n",
    "ml_cinh = gaussian_filter(ml_cinh, sigma=2.5)\n",
    "\n",
    "cape_alpha = np.clip(ml_cape / 100.0, 0, 1)\n",
    "\n",
    "proj = ccrs.LambertConformal(\n",
    "    central_longitude=-95.0, \n",
    "    standard_parallels=(25.0, 25.0)\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(16*2, 9*2), dpi=200)\n",
    "fig.patch.set_visible(False)\n",
    "ax = plt.axes(projection=proj)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.set_extent(\n",
    "    [-110, -80, \n",
    "     26, 45\n",
    "    ], crs=ccrs.PlateCarree()\n",
    ")\n",
    "\n",
    "ax.add_feature(\n",
    "    cfeature.OCEAN, \n",
    "    facecolor='#285970', \n",
    "    zorder=1\n",
    ")\n",
    "ax.add_feature(\n",
    "    cfeature.LAND, \n",
    "    facecolor='#757575', \n",
    "    zorder=1\n",
    ")\n",
    "ax.coastlines(\n",
    "    '50m', \n",
    "    color='k', \n",
    "    zorder=3\n",
    ")\n",
    "ax.add_feature(\n",
    "    cfeature.BORDERS, \n",
    "    color='k', \n",
    "    zorder=3\n",
    ")\n",
    "\n",
    "ax.add_feature(\n",
    "    cfeature.NaturalEarthFeature(\n",
    "        category='cultural',\n",
    "        name='admin_1_states_provinces_lines',\n",
    "        scale='50m',\n",
    "        facecolor='none',\n",
    "        edgecolor='k'\n",
    "    ), zorder=3\n",
    ")\n",
    "\n",
    "cm = ax.pcolormesh(\n",
    "    lon, \n",
    "    lat, \n",
    "    ml_cape, \n",
    "    alpha=cape_alpha,\n",
    "    edgecolors=None,\n",
    "    cmap='cmo.amp', \n",
    "    vmin=0, \n",
    "    vmax=5500,\n",
    "    shading=\"nearest\",\n",
    "    transform=ccrs.PlateCarree(), \n",
    "    rasterized=True,\n",
    "    zorder=2\n",
    ")\n",
    "\n",
    "cf = ax.contourf(\n",
    "    lon,\n",
    "    lat,\n",
    "    ml_cinh,\n",
    "    levels=[-250, -150, -100, -50, -25, -10],\n",
    "    cmap=\"Blues_r\",\n",
    "    extend=\"min\",\n",
    "    alpha=0.3,\n",
    "    vmin=-250, \n",
    "    vmax=0,\n",
    "    zorder=2,\n",
    "    hatches=['/','/','/','','',''],\n",
    "    transform=ccrs.PlateCarree(), \n",
    ")\n",
    "\n",
    "ct = ax.contour(\n",
    "    lon,\n",
    "    lat,\n",
    "    stp,\n",
    "    linewidths=[1, 2, 2.5, 4, 4],\n",
    "    levels=[0.5, 1.0, 3.0, 5.0, 10.0],\n",
    "    cmap=\"cmo.thermal\",\n",
    "    vmin=-20, \n",
    "    vmax=12,\n",
    "    zorder=3,\n",
    "    transform=ccrs.PlateCarree(), \n",
    ")\n",
    "\n",
    "ax.barbs(\n",
    "    lon[::25, ::25],\n",
    "    lat[::25, ::25], \n",
    "    ebwd_u[::25, ::25].values, \n",
    "    ebwd_v[::25, ::25].values,\n",
    "    barbcolor=\"k\",\n",
    "    edgecolor=\"w\",\n",
    "    length=8,\n",
    "    linewidth=1.5,\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    zorder=3,\n",
    ")\n",
    "\n",
    "## background for colorbars to improce label\n",
    "## legibility... \n",
    "rect = patches.Rectangle(\n",
    "    (0, 0), \n",
    "    0.15, \n",
    "    1, \n",
    "    transform=ax.transAxes, \n",
    "    facecolor=\"w\", \n",
    "    alpha=0.75\n",
    ")\n",
    "ax.add_patch(rect)\n",
    "\n",
    "divider = make_axes_locatable(ax)\n",
    "cbaxes = ax.inset_axes((0.075, 0.015, 0.025, 0.975), zorder=20)\n",
    "cb = fig.colorbar(cm, cax=cbaxes)\n",
    "cb.ax.tick_params(labelsize=24, labelcolor=\"k\")\n",
    "cb.set_label(\"ML CAPE (J/kg)\", labelpad=-98, fontsize=20)\n",
    "\n",
    "cbaxes2 = ax.inset_axes((0.005, 0.015, 0.025, 0.975), zorder=20)\n",
    "cb2 = fig.colorbar(cf, cax=cbaxes2)\n",
    "cb2.ax.tick_params(labelsize=24, labelcolor=\"k\")\n",
    "cb2.set_label(\"ML CINH (J/kg)\", labelpad=-101, fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plot.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "fig.clf()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43768208-20ca-41dc-ba4b-98e630921cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f95754-f3ba-4b5d-a67f-5683632445ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
