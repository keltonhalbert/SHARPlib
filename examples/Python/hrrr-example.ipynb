{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f886fb3-a3c1-4762-8414-6a73de4553c7",
   "metadata": {},
   "source": [
    "# **SHARPlib -- Working with Gridded Data (HRRR Example)**\n",
    "\n",
    "This tutorial notebook provides an example of how to use SHARPlib with 3D gridded data sources, such as the **High Resolution Rapid Refresh (HRRR)** model. This notebook leverages some advanced tools and topics, such as [reading remote GRIB2 data using `kerchunk`](https://nbviewer.org/gist/peterm790/92eb1df3d58ba41d3411f8a840be2452), and [parallelizing computations on chunked arrays using Dask](https://tutorial.dask.org/02_array.html). For the sake of brevity, this notebook will not go over these in any detail, but rather, show how they can be used with SHARPlib. \n",
    "\n",
    "## HRRR Data\n",
    "JSON reference files that map to a single GRIB2 file on Google Cloud are provided in this repository to provide access as a virtual Zarr store. There are 3 separate files for 3 separate groups of variables on different coordinate systems: `hrrr-hybrid.json` for hybrid vertical level data, `hrrr-2m.json` for fields defined at 2-meters AGL, and `hrrr-surface.json` for variables defined on the model surface. The surface pressure is read from the surface group, 2-meter temperature and specific humidity are read from the 2-meter group, and the remaining 3D data `[pressure, geopotential height, temperature, specific humidity, u-wind, v-wind]` are read from the hybrid level group. These reference files allow for lazy-loading and manipulation of datasets without having to download the full GRIB2 file.\n",
    "\n",
    "## Dask Client\n",
    "A Dask client is used to set up a \"local cluster\", using your computer's CPUs for parallel computation. ***It is highly recommended that you set the `n_workers` and `memory_limit` parameters to something appropriate for your system***. Setting `n_workers` or `memory_limit` to values that exceed what your system supports can cause significant slowdowns and even crashes.\n",
    "\n",
    "Once the client starts, you can click on the generated URL to see how your parallel tasks are being executed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7985aa-4d76-43d3-a001-674f0406b224",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dask throws some noise about \n",
    "## large task graphs. This is unavoidable\n",
    "## and clutters the output, so turn it off\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "import dask\n",
    "import logging\n",
    "import cmocean\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "from distributed import Client\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from cartopy import feature as cfeature\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from nwsspc.sharp.calc import constants\n",
    "from nwsspc.sharp.calc import parcel\n",
    "from nwsspc.sharp.calc import params\n",
    "from nwsspc.sharp.calc import thermo\n",
    "from nwsspc.sharp.calc import winds\n",
    "from nwsspc.sharp.calc import layer\n",
    "from nwsspc.sharp.calc import interp\n",
    "\n",
    "client = Client(\n",
    "    n_workers=8, \n",
    "    memory_limit='8GB', \n",
    "    silence_logs=logging.ERROR\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fa3ddc-6d55-4800-ac94-894816352ee8",
   "metadata": {},
   "source": [
    "## Dataset Chunking\n",
    "One of the ways we can control performance and parallelism is by controlling how the dataset is chunked. The chunk sizes control things like how much memory each task uses, how many tasks are generated, and task performance depending on how CPU cache-friendly the chunk strategy is.  \n",
    "\n",
    "First and foremost, the `hybrid` coordinate should be one, single chunk and not sub-divided. Our calculations with SHARPlib rely on contiguous, vertical profile arrays of data for computation, and splitting these into chunks in the vertical would break that requirement. This is effectively hard-coded in the call: `.chunk(dict(hybrid=-1, **chunks))`. \n",
    "\n",
    "Second, CPUs work most efficiently with linear, cache friendly data. Arrays are typically stored in memory using the C memory layout, meaning that an array shape of `(NZ, NY, NX)` is ordered linearly by `X` values. This means that, generally speaking, we can be the most efficient with our CPU cache by looping over data in a primarily \"row-oriented\" way. The current configuration has a larger chunk in the `x` dimension than the `y`. If the entire dataset were already loaded into memory, it can be considered advantageous to transpose the dataset to better take advantage of cache locality. \n",
    "\n",
    "Finally, an optimal chunk size can depend on the amount of \"expected work\". The most cache-friendly way to chunk would technically be to give a parallel thread access to an entire zonal row of profile data... however, computing convective parameters such as `CAPE, CINH,` and the `Effective Inflow Layer` have an implicit equatorial bias over the CONUS due to the meridional distribution of atmospheric moisture content. This means that if we were to give a process access to one or more zonal rows, the worker threads given poleward data have overall less work to do than threads that are more equatorial. Splitting things up into smaller chunks means that, if a light-workload thread finishes early, it can find another piece of work to contribute to.\n",
    "\n",
    "Depending on your system, number of cores, and amount of memory, you may want to adjust these values. I recommend adjusting the y-chunk first before x, but if you do adjust x, make sure its sufficiently large for the CPU to leverage linearity! If you don't quite believe it, experiment with what happens when you use a small x-chunk and a large y-chunk, and compare the timing...\n",
    "\n",
    "## Dataset Precision\n",
    "SHARPlib works using 32-bit floating point precision, while most of Python operates at 64-bit by default. A call to `.astype(\"float32\")` is made in order to convert all of the arrays to 32-bit precision up front. \n",
    "\n",
    "## Merging the Surface Variables and Lazy Loading\n",
    "Treating the GRIB2 data as a virtual Zarr store that is remote allows for some cool things, including dataset manipulation to merge the surface variables with the hybrid level data without having to actually download any data. This code effectively \"expands\" the `hybrid` dimension by 1 and sets the shelter level variables as the \"surface\", so that we can include 2-meter thermodynamics and 10-meter kinematics in our calculations. This keeps things consistent with how calculations are performed using radiosonde data. Once we fully construct a task graph, Dask will automatically download, merge, and convert our data using the instructions laid out here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1d541a-e359-4c64-9ac3-f73bdc25c5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "chunks = {\"y\": 128, \"x\": 256}\n",
    "\n",
    "ds_hybrid = xr.open_dataset(\n",
    "    \"hrrr-hybrid.json\", \n",
    "    engine=\"kerchunk\", \n",
    "    decode_timedelta=True\n",
    ")[[\"pres\", \"gh\", \"t\", \"q\", \"u\", \"v\"]]\n",
    "\n",
    "ds_2m = xr.open_dataset(\n",
    "    \"hrrr-2m.json\", \n",
    "    engine=\"kerchunk\", \n",
    "    decode_timedelta=True\n",
    ")[[\"t2m\", \"sh2\"]].squeeze().drop_vars([\"heightAboveGround\"])\n",
    "\n",
    "ds_10m = xr.open_dataset(\n",
    "    \"hrrr-10m.json\", \n",
    "    engine=\"kerchunk\", \n",
    "    decode_timedelta=True\n",
    ")[[\"u10\", \"v10\"]].squeeze().drop_vars([\"heightAboveGround\"])\n",
    "\n",
    "ds_sfc = xr.open_dataset(\n",
    "    \"hrrr-surface.json\", \n",
    "    engine=\"kerchunk\", \n",
    "    decode_timedelta=True\n",
    ")[[\"sp\", \"orog\"]].squeeze().drop_vars([\"surface\"])\n",
    "\n",
    "## Now merge the surface fields with the\n",
    "## hybrid level data\n",
    "sfc_level_coord = [0] \n",
    "sfc_vars = xr.Dataset({\n",
    "    'pres': ds_sfc['sp'],\n",
    "    'gh': ds_sfc['orog'],\n",
    "    't': ds_2m['t2m'],\n",
    "    'q': ds_2m['sh2'],\n",
    "    'u': ds_10m['u10'], \n",
    "    'v': ds_10m['v10'],\n",
    "})\n",
    "sfc_vars_3d = sfc_vars.expand_dims(hybrid=sfc_level_coord)\n",
    "profile_vars = ds_hybrid[['pres', 'gh', 't', 'q', 'u', 'v']]\n",
    "ds_merged = xr.concat(\n",
    "    [sfc_vars_3d, profile_vars], \n",
    "    dim='hybrid'\n",
    ") \\\n",
    "    .astype(\"float32\") \\\n",
    "    .chunk(dict(hybrid=-1, **chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24e02fa-810d-4d59-8a5e-b716ff84affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb15d70-5148-4d87-9c3c-860a2fc4e678",
   "metadata": {},
   "source": [
    "## Calling SHARPlib on Gridded Data\n",
    "SHARPlib is designed to work on vertical arrays of profile data (as mentioned in the chunking section), but our model data is a combination of 2D and 3D fields. Thankfully, the 2D fields have been merged into our 3D arrays, simplifying the process. There is also a need for computing additional array data, such as `water vapor mixing ratio`, `virtual temperature` and `dewpoint temperature`. While the aforementioned variables could reasonably be pre-computed on the 3D grids at a relatively low computational cost, other things such as `CAPE`, `CINH`, and the `Effective Inflow Layer` are much more computationally expensive... and iterating over the HRRR grid multiple times to compute each variable is usually more expensive than just computing everything at once. Once a gridpoint's vertical profile is loaded into memory, why not go ahead and get everything you need out of it?\n",
    "<hr>\n",
    "\n",
    "### Function Arguments\n",
    "The function arguments are going to be structured like so...\n",
    "\n",
    "- 1D Vertical Arrays: `[pres, hght, tmpk, spfh, u, v]`<br>\n",
    "\n",
    "The magic of how we get 1D and scalar values out of 3D and 2D arrays is elaborated on further in the next cell. The primary goal is to show that computational logic should be logically grouped into a \"per-profile\" basis. \n",
    "\n",
    "### SHARPlib Computations\n",
    "Within this function, we compute and return the following:\n",
    "\n",
    "- **Computed**\n",
    "    - Water Vapor Mixing Ratio\n",
    "    - Virtual Temperature \n",
    "    - Dewpoint Temperature\n",
    "<br>\n",
    "\n",
    "- **Computed and Returned**\n",
    "    - 100mb Mixed-layer CAPE\n",
    "    - 100mb Mixed-layer CINH\n",
    "    - Effective Layer Significant Tornado Parameter\n",
    "    - Effective Inflow Layer Bottom Height \n",
    "    - Effective Inflow Layer Top Height\n",
    "    - Effective Storm Relative Helicity\n",
    "    - Effective Bulk Wind Difference U-Component\n",
    "    - Effective Bulk Wind Difference V-Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae71bbe-90a4-40ac-bf7d-ad2a10dac365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_everything(\n",
    "    pres, hght, \n",
    "    tmpk, spfh, \n",
    "    uwin, vwin, \n",
    "):\n",
    "    def qc(val):\n",
    "        if (val == constants.MISSING): return np.nan\n",
    "        else: return val\n",
    "\n",
    "    # NWP data let specific humidity equal exactly zero. \n",
    "    # This can cause problems with dewpoint, so instead, \n",
    "    # just set it to a very small value...\n",
    "    mixr = thermo.mixratio(spfh)\n",
    "    mixr[mixr < constants.TOL] = constants.TOL\n",
    "    \n",
    "    theta = thermo.theta(pres, tmpk)\n",
    "    vtmp = thermo.virtual_temperature(tmpk, mixr)\n",
    "    dwpk = thermo.temperature_at_mixratio(mixr, pres)\n",
    "\n",
    "    # get the mixed-layer parcel\n",
    "    mix_lyr = layer.PressureLayer(\n",
    "        pres[0], pres[0] - 10000.0\n",
    "    )\n",
    "    mlpcl = parcel.Parcel.mixed_layer_parcel(\n",
    "        mix_lyr,\n",
    "        pres,\n",
    "        theta,\n",
    "        mixr\n",
    "    )\n",
    "\n",
    "    lifter = parcel.lifter_cm1()\n",
    "    lifter.ma_type = thermo.adiabat.pseudo_liq\n",
    "    lifter.converge = 0.15 # K\n",
    "    lifter.pressure_incr = 500.0 # 5 hPa\n",
    "    \n",
    "    # lift the parcel and get CAPE/CINH\n",
    "    pcl_vtmp = mlpcl.lift_parcel(lifter, pres)\n",
    "    pcl_buoy = thermo.buoyancy(pcl_vtmp, vtmp)\n",
    "    cape, cinh = mlpcl.cape_cinh(pres, hght, pcl_buoy)\n",
    "\n",
    "    # Get the LCL height in meters AGL\n",
    "    mllcl_hght = interp.interp_pressure(\n",
    "        mlpcl.lcl_pressure,\n",
    "        pres,\n",
    "        hght\n",
    "    ) - hght[0]\n",
    "\n",
    "    # Get the effective inflow layer\n",
    "    mupcl = parcel.Parcel()\n",
    "    eil = params.effective_inflow_layer(\n",
    "        lifter,\n",
    "        pres,\n",
    "        hght,\n",
    "        tmpk,\n",
    "        dwpk,\n",
    "        vtmp,\n",
    "        mupcl=mupcl\n",
    "    )\n",
    "    \n",
    "    eil_hght = layer.pressure_layer_to_height(\n",
    "        eil,\n",
    "        pres,\n",
    "        hght\n",
    "    )\n",
    "\n",
    "    # Get the storm relative helicity \n",
    "    storm_mtn = params.storm_motion_bunkers(\n",
    "        pres,\n",
    "        hght,\n",
    "        uwin,\n",
    "        vwin,\n",
    "        eil, \n",
    "        mupcl\n",
    "    )\n",
    "    esrh = winds.helicity(\n",
    "        eil,\n",
    "        storm_mtn,\n",
    "        pres,\n",
    "        uwin,\n",
    "        vwin\n",
    "    )\n",
    "\n",
    "    if mupcl.eql_pressure != constants.MISSING:\n",
    "        # Get the effective bulk wind difference\n",
    "        eql_hght = interp.interp_pressure(\n",
    "            mupcl.eql_pressure,\n",
    "            pres,\n",
    "            hght\n",
    "        )\n",
    "        depth = (eql_hght - eil_hght.bottom)*0.5\n",
    "        ebwd_lyr = layer.HeightLayer(\n",
    "            eil_hght.bottom, \n",
    "            float(eil_hght.bottom + depth)\n",
    "        )\n",
    "        ebwd_cmp = winds.wind_shear(\n",
    "            ebwd_lyr,\n",
    "            hght,\n",
    "            uwin,\n",
    "            vwin\n",
    "        )\n",
    "        ebwd = winds.vector_magnitude(ebwd_cmp.u, ebwd_cmp.v)\n",
    "    else:\n",
    "        ebwd_cmp = winds.WindComponents(constants.MISSING, constants.MISSING)\n",
    "        ebwd = constants.MISSING\n",
    "    \n",
    "    stp = params.significant_tornado_parameter(\n",
    "        mlpcl,\n",
    "        mllcl_hght,\n",
    "        esrh,\n",
    "        ebwd\n",
    "    )\n",
    "\n",
    "    return mlpcl.cape, mlpcl.cinh, stp, qc(eil_hght.bottom), qc(eil_hght.top), qc(esrh), qc(ebwd_cmp.u), qc(ebwd_cmp.v)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5899bc6-beb3-42e3-ac62-da43fb10ac0a",
   "metadata": {},
   "source": [
    "## Parallelizing Computations with Xarray and Dask\n",
    "This is the magic of how to parallelize the profile-based computation across an entire gridded dataset. It relies on two key variables: `input_core_dims` and `output_core_dims` that tell Xarray about how to decompose the input and reconstruct the output.\n",
    "\n",
    "### Input Core Dims\n",
    "This is a list of coordinates for each variable, ordered by argument order. The 6 arguments to `compute_everything` are the 1D arrays that we need to get from the 3D fields, and so the 6 values in `input_core_dims` are `[\"hybrid\"]`. The `hybrid` dimension is our vertical dimension from the GRIB2 file, so, we are telling Xarray that this dimension is \"core\" to our computation, and should be present for these arguments/arrays in our function. \n",
    "\n",
    "### Output Core Dims\n",
    "Similarly to `input_core_dims`, `output_core_dims` tells Xarray how to reconstruct the returned values from our output. In the case of `compute_everything`, we are returning 8 scalar values that should be assembled as 2D arrays. They have no \"core\" dimensions, since they are independent of any other neighbor or coordinate, so this is a list of empty lists. However, if we wanted to return a 3D array of parcel virtual temperature, we could add another output argument that would have an core dim of `[\"hybrid\"]`, much like the inputs. \n",
    "\n",
    "### Output DTypes\n",
    "Having the same length as the number of output variables, this tells Xarray the expected return type and precision. SHARPlib returns float32 types, so that's what we specify. \n",
    "\n",
    "### Other arguments\n",
    "The `vectorize=True` and `dask=\"parallelized` are required arguments in order to make this work. It coerces the function inputs to be numpy arrays (required to work with SHARPlib), and tells Dask that we want it to parallelize it. \n",
    "\n",
    "### Compute \n",
    "The return type from `apply_ufunc` is an object that tells Dask how to parallelize, but it does not compute a result intil `.compute()` is called. Before calling compute, for convenience, a new result `Dataset` is constructed for easy access. The `.compute()` command will then execute the task graph that Dask constructed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaef9d2-38f7-40a9-ac49-d70302ae7dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "input_core_dims = [\n",
    "    [\"hybrid\",], \n",
    "    [\"hybrid\",],\n",
    "    [\"hybrid\",],\n",
    "    [\"hybrid\",],\n",
    "    [\"hybrid\",],\n",
    "    [\"hybrid\",],\n",
    "]\n",
    "\n",
    "output_core_dims = [\n",
    "    [],[],\n",
    "    [],[],\n",
    "    [],[],\n",
    "    [],[],\n",
    "]\n",
    "\n",
    "output_dtypes = [\n",
    "    np.float32, np.float32, \n",
    "    np.float32, np.float32,\n",
    "    np.float32, np.float32,\n",
    "    np.float32, np.float32,\n",
    "]\n",
    "\n",
    "result_func = xr.apply_ufunc(\n",
    "    compute_everything, \n",
    "    ds_merged[\"pres\"], ds_merged[\"gh\"],\n",
    "    ds_merged[\"t\"], ds_merged[\"q\"],\n",
    "    ds_merged[\"u\"], ds_merged[\"v\"],\n",
    "    input_core_dims=input_core_dims,\n",
    "    output_core_dims=output_core_dims,\n",
    "    vectorize=True,\n",
    "    dask=\"parallelized\",\n",
    "    output_dtypes=output_dtypes,\n",
    ")\n",
    "\n",
    "output_names = [\n",
    "    \"mixed_layer_cape\", \"mixed_layer_cinh\", \n",
    "    \"effective_layer_stp\", \"effective_inflow_layer_height_bottom_agl\",\n",
    "    \"effective_inflow_layer_height_top_agl\", \"effective_srh\",\n",
    "    \"effective_bulk_wind_difference_u\", \"effective_bulk_wind_difference_v\",\n",
    "]\n",
    "\n",
    "result_dict = {name: data for name, data in zip(output_names, result_func)}\n",
    "result_ds = xr.Dataset(result_dict)\n",
    "result_ds = result_ds.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a693ed31-718c-4db0-be9f-53c60a7801de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6252d8f4-c279-444b-a310-75c57f415e16",
   "metadata": {},
   "source": [
    "## Visualizing the Data\n",
    "Computing complicated convective indices on 97,162,191 grid points is not so exciting unless you can do something with the data. Execute the cell below to generate a nice plot. Congratulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46545cad-fe6b-42bc-89de-545fcb1c47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lon = ds_2m.longitude.values\n",
    "lat = ds_2m.latitude.values\n",
    "\n",
    "stp = result_ds[\"effective_layer_stp\"]\n",
    "ml_cape = result_ds[\"mixed_layer_cape\"]\n",
    "ml_cinh = result_ds[\"mixed_layer_cinh\"]\n",
    "\n",
    "# convert from m/s to kts\n",
    "ebwd_u = result_ds[\"effective_bulk_wind_difference_u\"]*1.94384\n",
    "ebwd_v = result_ds[\"effective_bulk_wind_difference_v\"]*1.94384\n",
    "\n",
    "# contouring on a 3km grid can be messy \n",
    "# and time consuming without some smoothing\n",
    "stp = gaussian_filter(stp, sigma=2.5)\n",
    "ml_cinh = gaussian_filter(ml_cinh, sigma=2.5)\n",
    "\n",
    "# Make CAPE below 500 J/kg fade to transparent\n",
    "cape_alpha = np.clip(np.sqrt(ml_cape / 500.0), 0, 1)\n",
    "\n",
    "proj = ccrs.LambertConformal(\n",
    "    central_longitude=-95.0, \n",
    "    standard_parallels=(25.0, 25.0)\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(16*2, 9*2), dpi=200)\n",
    "fig.patch.set_visible(False)\n",
    "ax = plt.axes(projection=proj)\n",
    "ax.axis('off')\n",
    "\n",
    "ax.set_extent(\n",
    "    [-110, -80, \n",
    "     26, 45\n",
    "    ], crs=ccrs.PlateCarree()\n",
    ")\n",
    "\n",
    "ax.add_feature(\n",
    "    cfeature.OCEAN, \n",
    "    facecolor='#285970', \n",
    "    zorder=1\n",
    ")\n",
    "ax.add_feature(\n",
    "    cfeature.LAND, \n",
    "    facecolor='#757575', \n",
    "    zorder=1\n",
    ")\n",
    "ax.coastlines(\n",
    "    '50m', \n",
    "    color='k', \n",
    "    zorder=3\n",
    ")\n",
    "ax.add_feature(\n",
    "    cfeature.BORDERS, \n",
    "    color='k', \n",
    "    zorder=3\n",
    ")\n",
    "\n",
    "ax.add_feature(\n",
    "    cfeature.NaturalEarthFeature(\n",
    "        category='cultural',\n",
    "        name='admin_1_states_provinces_lines',\n",
    "        scale='50m',\n",
    "        facecolor='none',\n",
    "        edgecolor='k'\n",
    "    ), zorder=3\n",
    ")\n",
    "\n",
    "cm = ax.pcolormesh(\n",
    "    lon, \n",
    "    lat, \n",
    "    ml_cape, \n",
    "    alpha=cape_alpha,\n",
    "    edgecolors=None,\n",
    "    cmap='cmo.amp', \n",
    "    vmin=0, \n",
    "    vmax=5500,\n",
    "    shading=\"nearest\",\n",
    "    transform=ccrs.PlateCarree(), \n",
    "    rasterized=True,\n",
    "    zorder=2\n",
    ")\n",
    "\n",
    "cf = ax.contourf(\n",
    "    lon,\n",
    "    lat,\n",
    "    ml_cinh,\n",
    "    levels=[-250, -150, -100, -50, -25, -10],\n",
    "    cmap=\"Blues_r\",\n",
    "    extend=\"min\",\n",
    "    alpha=0.3,\n",
    "    vmin=-250, \n",
    "    vmax=0,\n",
    "    zorder=2,\n",
    "    hatches=['/','/','/','','',''],\n",
    "    transform=ccrs.PlateCarree(), \n",
    ")\n",
    "\n",
    "ct = ax.contour(\n",
    "    lon,\n",
    "    lat,\n",
    "    stp,\n",
    "    linewidths=[1, 2, 2.5, 4, 4],\n",
    "    levels=[0.5, 1.0, 3.0, 5.0, 10.0],\n",
    "    cmap=\"cmo.thermal\",\n",
    "    vmin=-20, \n",
    "    vmax=12,\n",
    "    zorder=3,\n",
    "    transform=ccrs.PlateCarree(), \n",
    ")\n",
    "\n",
    "ax.barbs(\n",
    "    lon[::25, ::25],\n",
    "    lat[::25, ::25], \n",
    "    ebwd_u[::25, ::25].values, \n",
    "    ebwd_v[::25, ::25].values,\n",
    "    barbcolor=\"k\",\n",
    "    edgecolor=\"w\",\n",
    "    length=8,\n",
    "    linewidth=1.5,\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    zorder=3,\n",
    ")\n",
    "\n",
    "# background for colorbars to \n",
    "# improve label legibility... \n",
    "rect = patches.Rectangle(\n",
    "    (0, 0), \n",
    "    0.15, \n",
    "    1, \n",
    "    transform=ax.transAxes, \n",
    "    facecolor=\"w\", \n",
    "    alpha=0.75\n",
    ")\n",
    "ax.add_patch(rect)\n",
    "\n",
    "divider = make_axes_locatable(ax)\n",
    "cbaxes = ax.inset_axes(\n",
    "    (0.075, 0.015, 0.025, 0.975), \n",
    "    zorder=20\n",
    ")\n",
    "cb = fig.colorbar(cm, cax=cbaxes)\n",
    "cb.ax.tick_params(labelsize=24, labelcolor=\"k\")\n",
    "cb.set_label(\n",
    "    \"ML CAPE (J/kg)\", \n",
    "    labelpad=-98, \n",
    "    fontsize=20\n",
    ")\n",
    "\n",
    "cbaxes2 = ax.inset_axes(\n",
    "    (0.005, 0.015, 0.025, 0.975), \n",
    "    zorder=20\n",
    ")\n",
    "cb2 = fig.colorbar(cf, cax=cbaxes2)\n",
    "cb2.ax.tick_params(labelsize=24, labelcolor=\"k\")\n",
    "cb2.set_label(\n",
    "    \"ML CINH (J/kg)\", \n",
    "    labelpad=-101, \n",
    "    fontsize=20\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plot.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "fig.clf()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d52660-ffe4-4606-b6ef-5fb41fc450de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
